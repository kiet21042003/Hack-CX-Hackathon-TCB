{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd011a3",
   "metadata": {},
   "source": [
    "# III. Advanced Modeling: Product Recommendation System\n",
    "\n",
    "## Expert-Level Modeling Strategy\n",
    "\n",
    "### Problem Reframing\n",
    "Instead of simple binary classification, we'll build a **product recommendation system** that:\n",
    "- Predicts adoption probability for each customer-product pair\n",
    "- Ranks products by adoption likelihood for each customer\n",
    "- Outputs top-3 product recommendations per customer\n",
    "- Achieves business-relevant performance metrics\n",
    "\n",
    "### Advanced Techniques\n",
    "1. **Multi-Target Learning**: Predict multiple products simultaneously\n",
    "2. **Ensemble Methods**: Combine multiple algorithms for robustness\n",
    "3. **Deep Feature Engineering**: Create business-meaningful features\n",
    "4. **Cross-Product Features**: Interaction terms between customer and product attributes\n",
    "5. **Time-Series Features**: Temporal patterns and seasonality\n",
    "6. **Collaborative Filtering**: User-item interaction patterns\n",
    "\n",
    "### Success Criteria\n",
    "- **Precision@3 > 60%**: At least 60% of top-3 recommendations should be adopted\n",
    "- **Recall@10 > 80%**: Capture 80% of actual adoptions in top-10 recommendations\n",
    "- **NDCG@5 > 0.7**: Strong ranking quality for business decisions\n",
    "- **Business Impact**: Measurable lift in conversion rates\n",
    "\n",
    "### Model Architecture\n",
    "We'll implement a **hybrid recommendation system** combining:\n",
    "- Content-based filtering (customer/product features)\n",
    "- Collaborative filtering (interaction patterns)\n",
    "- Deep neural networks for complex patterns\n",
    "- Gradient boosting for structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461e9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch available for deep learning\n",
      "‚úì Advanced modeling libraries imported successfully\n",
      "Available advanced libraries:\n",
      "  - XGBoost: True\n",
      "  - LightGBM: True\n",
      "  - PyTorch: True\n",
      "  - SHAP: True\n",
      "‚úì Advanced modeling libraries imported successfully\n",
      "Available advanced libraries:\n",
      "  - XGBoost: True\n",
      "  - LightGBM: True\n",
      "  - PyTorch: True\n",
      "  - SHAP: True\n"
     ]
    }
   ],
   "source": [
    "# Advanced Modeling - Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, roc_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, ndcg_score\n",
    ")\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Advanced Ensemble Methods\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available - will use LightGBM instead\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not available - will use alternative\")\n",
    "\n",
    "# Neural Networks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(\"PyTorch available for deep learning\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available - will use sklearn MLPClassifier\")\n",
    "    print(\"To install PyTorch: pip install torch\")\n",
    "\n",
    "# Model Interpretation\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"SHAP not available - will use feature importance instead\")\n",
    "\n",
    "# Utility Libraries\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"‚úì Advanced modeling libraries imported successfully\")\n",
    "print(f\"Available advanced libraries:\")\n",
    "print(f\"  - XGBoost: {XGBOOST_AVAILABLE}\")\n",
    "print(f\"  - LightGBM: {LIGHTGBM_AVAILABLE}\")\n",
    "print(f\"  - PyTorch: {PYTORCH_AVAILABLE}\")\n",
    "print(f\"  - SHAP: {SHAP_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee56b8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ LIBRARY INSTALLATION CHECK\n",
      "========================================\n",
      "‚úÖ All advanced libraries are available!\n",
      "\n",
      "üöÄ Proceeding with available libraries...\n"
     ]
    }
   ],
   "source": [
    "# Optional: Install missing libraries\n",
    "print(\"üì¶ LIBRARY INSTALLATION CHECK\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "missing_libs = []\n",
    "\n",
    "if not PYTORCH_AVAILABLE:\n",
    "    missing_libs.append(\"torch\")\n",
    "    print(\"‚ö†Ô∏è  PyTorch not found\")\n",
    "\n",
    "if not LIGHTGBM_AVAILABLE:\n",
    "    missing_libs.append(\"lightgbm\")\n",
    "    print(\"‚ö†Ô∏è  LightGBM not found\")\n",
    "\n",
    "if not SHAP_AVAILABLE:\n",
    "    missing_libs.append(\"shap\")\n",
    "    print(\"‚ö†Ô∏è  SHAP not found\")\n",
    "\n",
    "if missing_libs:\n",
    "    print(f\"\\nüí° To install missing libraries, run:\")\n",
    "    print(f\"   pip install {' '.join(missing_libs)}\")\n",
    "    print(f\"\\nüîÑ You can continue without these libraries - alternative models will be used\")\n",
    "else:\n",
    "    print(\"‚úÖ All advanced libraries are available!\")\n",
    "\n",
    "print(\"\\nüöÄ Proceeding with available libraries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460aa3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ADVANCED MODELING CONFIGURATION\n",
      "==================================================\n",
      "Target Precision@3: 60.0%\n",
      "Target Recall@10: 80.0%\n",
      "Target NDCG@5: 70.0%\n",
      "Cross-validation folds: 5\n",
      "Random state: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Constants\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Business Configuration\n",
    "TARGET_PRECISION_AT_3 = 0.60  # 60% precision for top-3 recommendations\n",
    "TARGET_RECALL_AT_10 = 0.80    # 80% recall for top-10 recommendations\n",
    "TARGET_NDCG_AT_5 = 0.70       # 70% NDCG for ranking quality\n",
    "\n",
    "# Model Configuration\n",
    "N_FOLDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.2\n",
    "MAX_FEATURES_TO_SELECT = 50\n",
    "\n",
    "print(\"üéØ ADVANCED MODELING CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Target Precision@3: {TARGET_PRECISION_AT_3:.1%}\")\n",
    "print(f\"Target Recall@10: {TARGET_RECALL_AT_10:.1%}\")\n",
    "print(f\"Target NDCG@5: {TARGET_NDCG_AT_5:.1%}\")\n",
    "print(f\"Cross-validation folds: {N_FOLDS}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8666f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LOADING DATA FOR ADVANCED MODELING\n",
      "==================================================\n",
      "Loaded datasets:\n",
      "  - Customers: (100000, 37)\n",
      "  - Products: (1000, 26)\n",
      "  - Adoption logs: (949650, 10)\n",
      "Loaded datasets:\n",
      "  - Customers: (100000, 37)\n",
      "  - Products: (1000, 26)\n",
      "  - Adoption logs: (949650, 10)\n",
      "  - Processed adoption: (949650, 17)\n",
      "  - Processed products: (1000, 81)\n",
      "\n",
      "Data Overview:\n",
      "  - Unique customers: 100,000\n",
      "  - Unique products: 1,000\n",
      "  - Total adoption events: 949,650\n",
      "  - Adoption rate: 25.1%\n",
      "  - Processed adoption: (949650, 17)\n",
      "  - Processed products: (1000, 81)\n",
      "\n",
      "Data Overview:\n",
      "  - Unique customers: 100,000\n",
      "  - Unique products: 1,000\n",
      "  - Total adoption events: 949,650\n",
      "  - Adoption rate: 25.1%\n"
     ]
    }
   ],
   "source": [
    "# Load and Prepare Data\n",
    "print(\"üìä LOADING DATA FOR ADVANCED MODELING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load all datasets\n",
    "customers = pd.read_csv('data/data_customers.csv')\n",
    "products = pd.read_csv('data/data_products.csv')\n",
    "adoption_logs = pd.read_csv('data/data_adoption_logs.csv')\n",
    "\n",
    "print(f\"Loaded datasets:\")\n",
    "print(f\"  - Customers: {customers.shape}\")\n",
    "print(f\"  - Products: {products.shape}\")\n",
    "print(f\"  - Adoption logs: {adoption_logs.shape}\")\n",
    "\n",
    "# Check for processed data\n",
    "try:\n",
    "    processed_adoption = pd.read_csv('data/processed_adoption_logs.csv')\n",
    "    processed_products = pd.read_csv('data/processed_products.csv')\n",
    "    PROCESSED_AVAILABLE = True\n",
    "    print(f\"  - Processed adoption: {processed_adoption.shape}\")\n",
    "    print(f\"  - Processed products: {processed_products.shape}\")\n",
    "except FileNotFoundError:\n",
    "    PROCESSED_AVAILABLE = False\n",
    "    print(\"  - No processed data found, will create from raw data\")\n",
    "\n",
    "# Basic data exploration\n",
    "print(f\"\\nData Overview:\")\n",
    "print(f\"  - Unique customers: {customers['user_id'].nunique():,}\")\n",
    "print(f\"  - Unique products: {products['product_id'].nunique():,}\")\n",
    "print(f\"  - Total adoption events: {adoption_logs.shape[0]:,}\")\n",
    "print(f\"  - Adoption rate: {adoption_logs['adopted'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7d6131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DEBUG: Checking column names\n",
      "Raw products columns: ['product_id', 'category', 'tier', 'apr', 'reward_type', 'reward_value', 'eligibility', 'tenor_months', 'risk_adj_margin', 'hist_conv_rate', 'hist_profit', 'budget_remaining', 'max_redemptions', 'offer_dates', 'launch_recency_days', 'compliance_tag', 'channels', 'target_segments', 'geo_applic', 'merchant_industry', 'cost_to_bank', 'expected_utility', 'cross_sell_score', 'bundle_depth', 'valid_window', 'popularity_trend']\n",
      "Processed products columns: ['apr', 'reward_value', 'tenor_months', 'risk_adj_margin', 'hist_conv_rate', 'hist_profit', 'budget_remaining', 'max_redemptions', 'offer_dates', 'launch_recency_days', 'cost_to_bank', 'expected_utility', 'cross_sell_score', 'bundle_depth', 'profitability_score', 'profit_segment', 'roi', 'is_competitive_apr', 'budget_utilization', 'offer_dates_year', 'offer_dates_month', 'offer_dates_quarter', 'offer_dates_dayofweek', 'offer_dates_is_weekend', 'product_id_frequency', 'category_DebitCard', 'category_FXTransfer', 'category_FixedDeposit', 'category_Insurance', 'category_InvestmentFund', 'category_Mortgage', 'category_Overdraft', 'category_PersonalLoan', 'category_SavingsAccount', 'tier_Infinite', 'tier_Platinum', 'tier_Signature', 'tier_Standard', 'reward_type_Discount', 'reward_type_Gift', 'reward_type_Miles', 'reward_type_Points', 'eligibility_CLV>80 & Tenure>=2y', \"eligibility_Holding<'CreditCard'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'DebitCard'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'FXTransfer'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'FixedDeposit'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'Insurance'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'InvestmentFund'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'Mortgage'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'Overdraft'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'PersonalLoan'>=False & ChurnRisk<0.4\", \"eligibility_Holding<'SavingsAccount'>=False & ChurnRisk<0.4\", \"eligibility_OccupationCode='TECH' & SalaryCreditedMonthly>=1000 USD\", 'compliance_tag_ConsumerLendingRule', 'compliance_tag_FXReg', 'compliance_tag_RegZ', 'compliance_tag_UsurySafe', 'channels_frequency', 'target_segments_frequency', 'geo_applic_Central', 'geo_applic_HCMC_Metro', 'geo_applic_Hanoi_Metro', 'geo_applic_North', 'geo_applic_South', 'merchant_industry_E-commerce', 'merchant_industry_Electronics', 'merchant_industry_Fuel', 'merchant_industry_Groceries', 'merchant_industry_Healthcare', 'merchant_industry_Hotels', 'merchant_industry_Restaurants', 'valid_window_frequency', 'popularity_trend_Rising', 'popularity_trend_Stable', 'conversion_tier_Good Performer', 'conversion_tier_Poor Performer', 'conversion_tier_Top Performer', 'age_segment_Mature', 'age_segment_New Launch', 'age_segment_Recent Launch']\n",
      "Adoption logs columns: ['adopted', 'tenure_days', 'recency_days', 'activity_intensity', 'monetary_volume', 'utilisation_ratio', 'reward_redemption_rate', 'risk_flag', 'user_id', 'product_id']\n",
      "Customers columns: ['user_id', 'age', 'occupation', 'income_tier', 'marital_status', 'household_size', 'preferred_language', 'products', 'tenure_years', 'avg_balance', 'cc_limit_util', 'mortgage_outstanding', 'investments_aum', 'monthly_salary', 'top_mcc', 'ecom_pos_ratio', 'overseas_share', 'avg_bill_pay_amt', 'cash_wd_freq', 'mobile_login_freq', 'days_since_push', 'preferred_channel', 'offer_ctr', 'offer_accepts', 'offer_fatigue', 'declined_offer_cat', 'day_time', 'season_flag', 'geo_region', 'weather', 'rt_spending_trigger', 'clv_score', 'churn_risk', 'propensity_scores', 'price_sensitivity', 'peer_cluster_vec', 'usage_journey']\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check column names\n",
    "print(\"\\nüîç DEBUG: Checking column names\")\n",
    "print(f\"Raw products columns: {list(products.columns)}\")\n",
    "print(f\"Processed products columns: {list(processed_products.columns)}\")\n",
    "print(f\"Adoption logs columns: {list(adoption_logs.columns)}\")\n",
    "print(f\"Customers columns: {list(customers.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165a016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ADVANCED FEATURE ENGINEERING\n",
      "==================================================\n",
      "Starting comprehensive feature engineering...\n",
      "Using existing adoption logs as base matrix (memory efficient)...\n",
      "Base interaction matrix: 949,650 customer-product pairs\n",
      "Adoption rate: 25.1%\n",
      "Active customers: 100,000\n",
      "Active products: 1,000\n",
      "Adding strategic negative samples...\n",
      "Active customers: 100,000\n",
      "Active products: 1,000\n",
      "Adding strategic negative samples...\n",
      "Added 50,000 negative samples\n",
      "Final matrix: 999,650 customer-product pairs\n",
      "Final adoption rate: 23.8%\n",
      "\n",
      "üßë‚Äçüíº Engineering customer features...\n",
      "Added 50,000 negative samples\n",
      "Final matrix: 999,650 customer-product pairs\n",
      "Final adoption rate: 23.8%\n",
      "\n",
      "üßë‚Äçüíº Engineering customer features...\n",
      "  ‚úì Customer features: 41 columns\n",
      "\n",
      "üì¶ Engineering product features...\n",
      "  ‚úì Customer features: 41 columns\n",
      "\n",
      "üì¶ Engineering product features...\n",
      "  ‚úì Product features: 88 columns\n",
      "\n",
      "üîó Creating interaction features...\n",
      "  ‚úì Product features: 88 columns\n",
      "\n",
      "üîó Creating interaction features...\n",
      "  ‚úì Interaction features created: 134 total columns\n",
      "\n",
      "‚úÖ FEATURE ENGINEERING COMPLETED\n",
      "Final feature matrix: (999650, 134)\n",
      "Adoption rate: 23.8%\n",
      "  ‚úì Interaction features created: 134 total columns\n",
      "\n",
      "‚úÖ FEATURE ENGINEERING COMPLETED\n",
      "Final feature matrix: (999650, 134)\n",
      "Adoption rate: 23.8%\n"
     ]
    }
   ],
   "source": [
    "# Advanced Feature Engineering\n",
    "print(\"üîß ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def create_customer_product_matrix():\n",
    "    \"\"\"Create optimized customer-product interaction matrix\"\"\"\n",
    "    \n",
    "    # Instead of Cartesian product, use existing adoption logs as base\n",
    "    print(\"Using existing adoption logs as base matrix (memory efficient)...\")\n",
    "    \n",
    "    # Start with existing interactions\n",
    "    base_matrix = adoption_logs[['user_id', 'product_id', 'adopted']].copy()\n",
    "    \n",
    "    print(f\"Base interaction matrix: {base_matrix.shape[0]:,} customer-product pairs\")\n",
    "    print(f\"Adoption rate: {base_matrix['adopted'].mean():.1%}\")\n",
    "    \n",
    "    # Optional: Add some negative samples strategically (not all combinations)\n",
    "    # This prevents memory explosion while maintaining model effectiveness\n",
    "    \n",
    "    # Get customers and products with interactions\n",
    "    active_customers = base_matrix['user_id'].unique()\n",
    "    active_products = base_matrix['product_id'].unique()\n",
    "    \n",
    "    print(f\"Active customers: {len(active_customers):,}\")\n",
    "    print(f\"Active products: {len(active_products):,}\")\n",
    "    \n",
    "    # Create a small sample of non-existing pairs for better model training\n",
    "    print(\"Adding strategic negative samples...\")\n",
    "    \n",
    "    # Sample some customers and products for negative examples\n",
    "    sample_size = min(50000, len(active_customers) * 10)  # Limit to prevent memory issues\n",
    "    \n",
    "    # Create negative samples\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    negative_samples = []\n",
    "    \n",
    "    existing_pairs = set(zip(base_matrix['user_id'], base_matrix['product_id']))\n",
    "    \n",
    "    attempts = 0\n",
    "    while len(negative_samples) < sample_size and attempts < sample_size * 3:\n",
    "        customer = np.random.choice(active_customers)\n",
    "        product = np.random.choice(active_products)\n",
    "        \n",
    "        if (customer, product) not in existing_pairs:\n",
    "            negative_samples.append({\n",
    "                'user_id': customer,\n",
    "                'product_id': product,\n",
    "                'adopted': 0\n",
    "            })\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    if negative_samples:\n",
    "        negative_df = pd.DataFrame(negative_samples)\n",
    "        base_matrix = pd.concat([base_matrix, negative_df], ignore_index=True)\n",
    "        print(f\"Added {len(negative_samples):,} negative samples\")\n",
    "    \n",
    "    print(f\"Final matrix: {base_matrix.shape[0]:,} customer-product pairs\")\n",
    "    print(f\"Final adoption rate: {base_matrix['adopted'].mean():.1%}\")\n",
    "    \n",
    "    return base_matrix\n",
    "\n",
    "def engineer_customer_features(df):\n",
    "    \"\"\"Create advanced customer features\"\"\"\n",
    "    print(\"\\nüßë‚Äçüíº Engineering customer features...\")\n",
    "    \n",
    "    customer_features = customers.copy()\n",
    "    \n",
    "    # Behavioral features from adoption history\n",
    "    adoption_stats = adoption_logs.groupby('user_id').agg({\n",
    "        'adopted': ['count', 'sum', 'mean'],\n",
    "        'product_id': 'nunique'\n",
    "    }).round(4)\n",
    "    \n",
    "    adoption_stats.columns = [\n",
    "        'total_interactions', 'total_adoptions', 'adoption_rate', 'unique_products_tried'\n",
    "    ]\n",
    "    \n",
    "    # Customer lifecycle features\n",
    "    if 'signup_date' in customers.columns:\n",
    "        customer_features['signup_date'] = pd.to_datetime(customer_features['signup_date'])\n",
    "        customer_features['days_since_signup'] = (\n",
    "            pd.Timestamp.now() - customer_features['signup_date']\n",
    "        ).dt.days\n",
    "        \n",
    "        # Customer maturity segments\n",
    "        customer_features['customer_maturity'] = pd.cut(\n",
    "            customer_features['days_since_signup'],\n",
    "            bins=[0, 30, 90, 365, float('inf')],\n",
    "            labels=['New', 'Recent', 'Established', 'Veteran']\n",
    "        )\n",
    "    \n",
    "    # Financial profile features\n",
    "    if 'annual_income' in customer_features.columns and 'account_balance' in customer_features.columns:\n",
    "        customer_features['balance_to_income_ratio'] = (\n",
    "            customer_features['account_balance'] / customer_features['annual_income']\n",
    "        ).fillna(0)\n",
    "        \n",
    "        customer_features['financial_capacity'] = pd.cut(\n",
    "            customer_features['balance_to_income_ratio'],\n",
    "            bins=[0, 0.1, 0.5, 1.0, float('inf')],\n",
    "            labels=['Low', 'Medium', 'High', 'Very High']\n",
    "        )\n",
    "    \n",
    "    # Merge adoption statistics\n",
    "    customer_features = customer_features.merge(adoption_stats, on='user_id', how='left')\n",
    "    customer_features = customer_features.fillna(0)\n",
    "    \n",
    "    print(f\"  ‚úì Customer features: {customer_features.shape[1]} columns\")\n",
    "    return customer_features\n",
    "\n",
    "def engineer_product_features(df):\n",
    "    \"\"\"Create advanced product features\"\"\"\n",
    "    print(\"\\nüì¶ Engineering product features...\")\n",
    "    \n",
    "    if PROCESSED_AVAILABLE:\n",
    "        product_features = processed_products.copy()\n",
    "        # Add product_id back if it's missing\n",
    "        if 'product_id' not in product_features.columns:\n",
    "            product_features['product_id'] = products['product_id'].values\n",
    "    else:\n",
    "        product_features = products.copy()\n",
    "    \n",
    "    # Product popularity features\n",
    "    product_stats = adoption_logs.groupby('product_id').agg({\n",
    "        'adopted': ['count', 'sum', 'mean'],\n",
    "        'user_id': 'nunique'\n",
    "    }).round(4)\n",
    "    \n",
    "    product_stats.columns = [\n",
    "        'total_exposures', 'total_adoptions', 'adoption_rate', 'unique_customers'\n",
    "    ]\n",
    "    \n",
    "    # Reset index to make product_id a column\n",
    "    product_stats = product_stats.reset_index()\n",
    "    \n",
    "    # Product performance segments\n",
    "    product_stats['performance_tier'] = pd.cut(\n",
    "        product_stats['adoption_rate'],\n",
    "        bins=[0, 0.1, 0.3, 0.6, 1.0],\n",
    "        labels=['Poor', 'Average', 'Good', 'Excellent']\n",
    "    )\n",
    "    \n",
    "    # Market penetration\n",
    "    total_customers = customers['user_id'].nunique()\n",
    "    product_stats['market_penetration'] = product_stats['unique_customers'] / total_customers\n",
    "    \n",
    "    # Merge with product features\n",
    "    product_features = product_features.merge(product_stats, on='product_id', how='left')\n",
    "    \n",
    "    # Handle missing values properly for different column types\n",
    "    for col in product_features.columns:\n",
    "        if product_features[col].dtype.name == 'category':\n",
    "            # For categorical columns, fill with the mode or first category\n",
    "            if product_features[col].isnull().any():\n",
    "                mode_val = product_features[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    product_features[col] = product_features[col].fillna(mode_val[0])\n",
    "                else:\n",
    "                    # If no mode, use first category\n",
    "                    product_features[col] = product_features[col].cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "        elif pd.api.types.is_numeric_dtype(product_features[col]):\n",
    "            # For numeric columns, fill with 0\n",
    "            product_features[col] = product_features[col].fillna(0)\n",
    "        else:\n",
    "            # For other types, fill with 'Unknown' or appropriate default\n",
    "            product_features[col] = product_features[col].fillna('Unknown')\n",
    "    \n",
    "    print(f\"  ‚úì Product features: {product_features.shape[1]} columns\")\n",
    "    return product_features\n",
    "\n",
    "def create_interaction_features(df, customer_features, product_features):\n",
    "    \"\"\"Create customer-product interaction features\"\"\"\n",
    "    print(\"\\nüîó Creating interaction features...\")\n",
    "    \n",
    "    # Merge customer and product features\n",
    "    enriched_df = df.merge(customer_features, on='user_id', how='left')\n",
    "    enriched_df = enriched_df.merge(product_features, on='product_id', how='left')\n",
    "    \n",
    "    # Create interaction features\n",
    "    numeric_customer_cols = customer_features.select_dtypes(include=[np.number]).columns\n",
    "    numeric_product_cols = product_features.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Remove ID columns\n",
    "    numeric_customer_cols = [col for col in numeric_customer_cols if 'id' not in col.lower()]\n",
    "    numeric_product_cols = [col for col in numeric_product_cols if 'id' not in col.lower()]\n",
    "    \n",
    "    # Create key interaction features (limit to avoid explosion)\n",
    "    key_interactions = [\n",
    "        ('annual_income', 'price'),\n",
    "        ('account_balance', 'price'), \n",
    "        ('adoption_rate_x', 'adoption_rate_y'),  # customer vs product adoption rates\n",
    "        ('unique_products_tried', 'market_penetration')\n",
    "    ]\n",
    "    \n",
    "    for customer_col, product_col in key_interactions:\n",
    "        if customer_col in enriched_df.columns and product_col in enriched_df.columns:\n",
    "            # Ratio features\n",
    "            interaction_name = f\"{customer_col}_to_{product_col}_ratio\"\n",
    "            enriched_df[interaction_name] = (\n",
    "                enriched_df[customer_col] / (enriched_df[product_col] + 1e-8)\n",
    "            )\n",
    "            \n",
    "            # Product features\n",
    "            product_name = f\"{customer_col}_times_{product_col}\"\n",
    "            enriched_df[product_name] = enriched_df[customer_col] * enriched_df[product_col]\n",
    "    \n",
    "    print(f\"  ‚úì Interaction features created: {enriched_df.shape[1]} total columns\")\n",
    "    return enriched_df\n",
    "\n",
    "# Execute feature engineering\n",
    "print(\"Starting comprehensive feature engineering...\")\n",
    "\n",
    "# Create customer-product matrix\n",
    "cp_matrix = create_customer_product_matrix()\n",
    "\n",
    "# Engineer features\n",
    "customer_features = engineer_customer_features(cp_matrix)\n",
    "product_features = engineer_product_features(cp_matrix)\n",
    "\n",
    "# Create final feature matrix\n",
    "feature_matrix = create_interaction_features(cp_matrix, customer_features, product_features)\n",
    "\n",
    "print(f\"\\n‚úÖ FEATURE ENGINEERING COMPLETED\")\n",
    "print(f\"Final feature matrix: {feature_matrix.shape}\")\n",
    "print(f\"Adoption rate: {feature_matrix['adopted'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200dd60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ADVANCED DATA PREPROCESSING (ULTRA MEMORY EFFICIENT)\n",
      "==================================================\n",
      "Original dataset: 999,650 rows, 134 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 2041.0 MB\n",
      "\n",
      "‚ö†Ô∏è  Dataset too large for memory. Sampling 200,000 rows...\n",
      "Sampled dataset: 200,000 rows\n",
      "Adoption rate: 24.0%\n",
      "Sampled dataset: 200,000 rows\n",
      "Adoption rate: 24.0%\n",
      "Memory usage: 408.3 MB\n",
      "Processing 18 categorical columns...\n",
      "  ‚úì Label encoded occupation\n",
      "  ‚úì Label encoded income_tier\n",
      "  ‚úì Label encoded marital_status\n",
      "Memory usage: 408.3 MB\n",
      "Processing 18 categorical columns...\n",
      "  ‚úì Label encoded occupation\n",
      "  ‚úì Label encoded income_tier\n",
      "  ‚úì Label encoded marital_status\n",
      "  ‚úì Label encoded preferred_language\n",
      "  ‚úì Label encoded products\n",
      "  ‚úì Dropped 13 categorical columns to save memory\n",
      "  ‚úì Skipping correlation analysis to save memory\n",
      "  ‚úì Label encoded preferred_language\n",
      "  ‚úì Label encoded products\n",
      "  ‚úì Dropped 13 categorical columns to save memory\n",
      "  ‚úì Skipping correlation analysis to save memory\n",
      "\n",
      "üìä FINAL MODELING DATASET\n",
      "Features: 118\n",
      "Samples: 200,000\n",
      "Positive samples: 48,000 (24.0%)\n",
      "Memory usage: 53.6 MB\n",
      "\n",
      "Data quality checks:\n",
      "  - Missing values: 0\n",
      "  - Infinite values: 0\n",
      "\n",
      "üìä FINAL MODELING DATASET\n",
      "Features: 118\n",
      "Samples: 200,000\n",
      "Positive samples: 48,000 (24.0%)\n",
      "Memory usage: 53.6 MB\n",
      "\n",
      "Data quality checks:\n",
      "  - Missing values: 0\n",
      "  - Infinite values: 0\n",
      "  - Feature variance > 0: 111/118\n",
      "\n",
      "‚úì Memory optimization completed\n",
      "  - Feature variance > 0: 111/118\n",
      "\n",
      "‚úì Memory optimization completed\n"
     ]
    }
   ],
   "source": [
    "# Advanced Data Preprocessing (Ultra Memory Efficient)\n",
    "print(\"üîÑ ADVANCED DATA PREPROCESSING (ULTRA MEMORY EFFICIENT)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check available memory and reduce dataset size if needed\n",
    "print(f\"Original dataset: {feature_matrix.shape[0]:,} rows, {feature_matrix.shape[1]} columns\")\n",
    "print(f\"Memory usage: {feature_matrix.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Sample the dataset to manageable size for memory constraints\n",
    "max_rows = 200000  # Reduce to 200k rows to fit in memory\n",
    "if feature_matrix.shape[0] > max_rows:\n",
    "    print(f\"\\n‚ö†Ô∏è  Dataset too large for memory. Sampling {max_rows:,} rows...\")\n",
    "    \n",
    "    # Stratified sampling to maintain class balance\n",
    "    sampled_positive = feature_matrix[feature_matrix['adopted'] == 1].sample(\n",
    "        n=min(int(max_rows * 0.24), feature_matrix[feature_matrix['adopted'] == 1].shape[0]), \n",
    "        random_state=42\n",
    "    )\n",
    "    sampled_negative = feature_matrix[feature_matrix['adopted'] == 0].sample(\n",
    "        n=max_rows - len(sampled_positive), \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    sampled_data = pd.concat([sampled_positive, sampled_negative], ignore_index=True)\n",
    "    sampled_data = sampled_data.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "    \n",
    "    print(f\"Sampled dataset: {sampled_data.shape[0]:,} rows\")\n",
    "    print(f\"Adoption rate: {sampled_data['adopted'].mean():.1%}\")\n",
    "    print(f\"Memory usage: {sampled_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    feature_matrix_working = sampled_data.copy()\n",
    "else:\n",
    "    feature_matrix_working = feature_matrix.copy()\n",
    "\n",
    "def preprocess_for_modeling_minimal(df):\n",
    "    \"\"\"Minimal preprocessing to avoid memory issues\"\"\"\n",
    "    \n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Handle only essential categorical encoding\n",
    "    categorical_cols = processed_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col not in ['user_id', 'product_id']]\n",
    "    \n",
    "    print(f\"Processing {len(categorical_cols)} categorical columns...\")\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols[:5]:  # Limit to first 5 to save memory\n",
    "        if col in processed_df.columns:\n",
    "            try:\n",
    "                le = LabelEncoder()\n",
    "                processed_df[col] = le.fit_transform(processed_df[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "                print(f\"  ‚úì Label encoded {col}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Skipped {col}: {e}\")\n",
    "                processed_df = processed_df.drop(columns=[col])\n",
    "    \n",
    "    # Drop remaining categorical columns to save memory\n",
    "    remaining_cats = [col for col in categorical_cols[5:] if col in processed_df.columns]\n",
    "    if remaining_cats:\n",
    "        processed_df = processed_df.drop(columns=remaining_cats)\n",
    "        print(f\"  ‚úì Dropped {len(remaining_cats)} categorical columns to save memory\")\n",
    "    \n",
    "    # Handle missing values minimally\n",
    "    numeric_cols = processed_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        if processed_df[col].isnull().sum() > 0:\n",
    "            if col.endswith('_rate') or col.endswith('_ratio'):\n",
    "                processed_df[col] = processed_df[col].fillna(0)\n",
    "            else:\n",
    "                processed_df[col] = processed_df[col].fillna(processed_df[col].median())\n",
    "    \n",
    "    # Skip correlation removal to save memory\n",
    "    print(\"  ‚úì Skipping correlation analysis to save memory\")\n",
    "    \n",
    "    # Convert to smaller data types\n",
    "    for col in processed_df.select_dtypes(include=['float64']).columns:\n",
    "        if col not in ['user_id', 'product_id']:\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], downcast='float')\n",
    "    \n",
    "    for col in processed_df.select_dtypes(include=['int64']).columns:\n",
    "        if col not in ['user_id', 'product_id']:\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], downcast='integer')\n",
    "    \n",
    "    return processed_df, label_encoders\n",
    "\n",
    "# Apply minimal preprocessing\n",
    "modeling_data, encoders = preprocess_for_modeling_minimal(feature_matrix_working)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in modeling_data.columns if col not in ['user_id', 'product_id', 'adopted']]\n",
    "X = modeling_data[feature_cols]\n",
    "y = modeling_data['adopted'].astype('uint8')\n",
    "\n",
    "print(f\"\\nüìä FINAL MODELING DATASET\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]:,}\")\n",
    "print(f\"Positive samples: {y.sum():,} ({y.mean():.1%})\")\n",
    "print(f\"Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Check data quality\n",
    "print(f\"\\nData quality checks:\")\n",
    "print(f\"  - Missing values: {X.isnull().sum().sum()}\")\n",
    "print(f\"  - Infinite values: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"  - Feature variance > 0: {(X.var() > 0).sum()}/{X.shape[1]}\")\n",
    "\n",
    "# Clear intermediate variables to free memory\n",
    "del feature_matrix_working\n",
    "if 'sampled_data' in locals():\n",
    "    del sampled_data\n",
    "    del sampled_positive, sampled_negative\n",
    "\n",
    "print(\"\\n‚úì Memory optimization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed9f356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ INTELLIGENT DATA SPLITTING\n",
      "==================================================\n",
      "User-based splits:\n",
      "  Train users: 53,382 (60.0%)\n",
      "  Val users: 17,794 (20.0%)\n",
      "  Test users: 17,795 (20.0%)\n",
      "\n",
      "Interaction splits:\n",
      "  Train: 119,975 interactions (23.8% positive)\n",
      "  Val: 39,926 interactions (24.4% positive)\n",
      "  Test: 40,099 interactions (24.1% positive)\n",
      "User-based splits:\n",
      "  Train users: 53,382 (60.0%)\n",
      "  Val users: 17,794 (20.0%)\n",
      "  Test users: 17,795 (20.0%)\n",
      "\n",
      "Interaction splits:\n",
      "  Train: 119,975 interactions (23.8% positive)\n",
      "  Val: 39,926 interactions (24.4% positive)\n",
      "  Test: 40,099 interactions (24.1% positive)\n",
      "‚úÖ Data splitting and scaling completed successfully\n",
      "‚úÖ Data splitting and scaling completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Smart Train-Validation-Test Split\n",
    "print(\"üéØ INTELLIGENT DATA SPLITTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def create_smart_splits(X, y, modeling_data):\n",
    "    \"\"\"Create stratified splits with business logic\"\"\"\n",
    "    \n",
    "    # For recommendation systems, we need to ensure both customers and products\n",
    "    # are represented across splits\n",
    "    \n",
    "    users = modeling_data['user_id'].unique()\n",
    "    products = modeling_data['product_id'].unique()\n",
    "    \n",
    "    # Split users (not individual interactions)\n",
    "    train_users, temp_users = train_test_split(\n",
    "        users, test_size=0.4, random_state=RANDOM_STATE\n",
    "    )\n",
    "    val_users, test_users = train_test_split(\n",
    "        temp_users, test_size=0.5, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Create splits based on users\n",
    "    train_mask = modeling_data['user_id'].isin(train_users)\n",
    "    val_mask = modeling_data['user_id'].isin(val_users)\n",
    "    test_mask = modeling_data['user_id'].isin(test_users)\n",
    "    \n",
    "    X_train = X[train_mask]\n",
    "    y_train = y[train_mask]\n",
    "    X_val = X[val_mask]\n",
    "    y_val = y[val_mask]\n",
    "    X_test = X[test_mask]\n",
    "    y_test = y[test_mask]\n",
    "    \n",
    "    print(f\"User-based splits:\")\n",
    "    print(f\"  Train users: {len(train_users):,} ({len(train_users)/len(users):.1%})\")\n",
    "    print(f\"  Val users: {len(val_users):,} ({len(val_users)/len(users):.1%})\")\n",
    "    print(f\"  Test users: {len(test_users):,} ({len(test_users)/len(users):.1%})\")\n",
    "    \n",
    "    print(f\"\\nInteraction splits:\")\n",
    "    print(f\"  Train: {X_train.shape[0]:,} interactions ({y_train.mean():.1%} positive)\")\n",
    "    print(f\"  Val: {X_val.shape[0]:,} interactions ({y_val.mean():.1%} positive)\")\n",
    "    print(f\"  Test: {X_test.shape[0]:,} interactions ({y_test.mean():.1%} positive)\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "# Create splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, train_mask, val_mask, test_mask = create_smart_splits(\n",
    "    X, y, modeling_data\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Data splitting and scaling completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f8f759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ ADVANCED MODEL DEVELOPMENT\n",
      "==================================================\n",
      "üîß Training advanced ensemble...\n",
      "  ‚úì Selected 28 most important features\n",
      "  üèãÔ∏è Training logistic...\n",
      "  ‚úì Selected 28 most important features\n",
      "  üèãÔ∏è Training logistic...\n",
      "    Validation AUC: 0.7146\n",
      "  üèãÔ∏è Training random_forest...\n",
      "    Validation AUC: 0.7146\n",
      "  üèãÔ∏è Training random_forest...\n",
      "    Validation AUC: 0.7186\n",
      "  üèãÔ∏è Training gradient_boosting...\n",
      "    Validation AUC: 0.7186\n",
      "  üèãÔ∏è Training gradient_boosting...\n",
      "    Validation AUC: 0.7161\n",
      "  üèãÔ∏è Training lightgbm...\n",
      "    Validation AUC: 0.7161\n",
      "  üèãÔ∏è Training lightgbm...\n",
      "    Validation AUC: 0.7198\n",
      "  üèãÔ∏è Training pytorch_nn...\n",
      "    Validation AUC: 0.7198\n",
      "  üèãÔ∏è Training pytorch_nn...\n",
      "    Epoch 0/30, Loss: 0.6051\n",
      "    Epoch 0/30, Loss: 0.6051\n",
      "    Epoch 10/30, Loss: 0.4915\n",
      "    Epoch 10/30, Loss: 0.4915\n",
      "    Epoch 20/30, Loss: 0.4874\n",
      "    Epoch 20/30, Loss: 0.4874\n",
      "    Validation AUC: 0.7205\n",
      "\n",
      "  üìä Ensemble weights:\n",
      "    logistic: 0.199\n",
      "    random_forest: 0.200\n",
      "    gradient_boosting: 0.200\n",
      "    lightgbm: 0.201\n",
      "    pytorch_nn: 0.201\n",
      "‚úÖ Advanced ensemble model training completed\n",
      "    Validation AUC: 0.7205\n",
      "\n",
      "  üìä Ensemble weights:\n",
      "    logistic: 0.199\n",
      "    random_forest: 0.200\n",
      "    gradient_boosting: 0.200\n",
      "    lightgbm: 0.201\n",
      "    pytorch_nn: 0.201\n",
      "‚úÖ Advanced ensemble model training completed\n"
     ]
    }
   ],
   "source": [
    "# Advanced Model Development\n",
    "print(\"ü§ñ ADVANCED MODEL DEVELOPMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PyTorch Neural Network Class\n",
    "if PYTORCH_AVAILABLE:\n",
    "    class PyTorchBinaryClassifier(nn.Module):\n",
    "        def __init__(self, input_size, hidden_sizes=[128, 64, 32], dropout_rate=0.3):\n",
    "            super(PyTorchBinaryClassifier, self).__init__()\n",
    "            \n",
    "            layers = []\n",
    "            prev_size = input_size\n",
    "            \n",
    "            for hidden_size in hidden_sizes:\n",
    "                layers.append(nn.Linear(prev_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "                prev_size = hidden_size\n",
    "            \n",
    "            layers.append(nn.Linear(prev_size, 1))\n",
    "            layers.append(nn.Sigmoid())\n",
    "            \n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    class PyTorchWrapper:\n",
    "        \"\"\"Wrapper to make PyTorch model compatible with sklearn interface\"\"\"\n",
    "        \n",
    "        def __init__(self, input_size, hidden_sizes=[128, 64, 32], learning_rate=0.001, epochs=50, batch_size=512):\n",
    "            self.input_size = input_size\n",
    "            self.hidden_sizes = hidden_sizes\n",
    "            self.learning_rate = learning_rate\n",
    "            self.epochs = epochs\n",
    "            self.batch_size = batch_size\n",
    "            self.model = None\n",
    "            self.scaler = StandardScaler()\n",
    "            \n",
    "        def fit(self, X, y):\n",
    "            # Scale features\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            X_tensor = torch.FloatTensor(X_scaled)\n",
    "            y_tensor = torch.FloatTensor(y.values if hasattr(y, 'values') else y).reshape(-1, 1)\n",
    "            \n",
    "            # Create dataset\n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            \n",
    "            # Initialize model\n",
    "            self.model = PyTorchBinaryClassifier(X_scaled.shape[1], self.hidden_sizes)\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "            \n",
    "            # Training loop\n",
    "            self.model.train()\n",
    "            for epoch in range(self.epochs):\n",
    "                total_loss = 0\n",
    "                for batch_X, batch_y in dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                if epoch % 10 == 0:\n",
    "                    avg_loss = total_loss / len(dataloader)\n",
    "                    print(f\"    Epoch {epoch}/{self.epochs}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            return self\n",
    "        \n",
    "        def predict(self, X):\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            X_tensor = torch.FloatTensor(X_scaled)\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(X_tensor)\n",
    "                predictions = (outputs.numpy() > 0.5).astype(int).flatten()\n",
    "            \n",
    "            return predictions\n",
    "        \n",
    "        def predict_proba(self, X):\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            X_tensor = torch.FloatTensor(X_scaled)\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(X_tensor)\n",
    "                proba_positive = outputs.numpy().flatten()\n",
    "                proba_negative = 1 - proba_positive\n",
    "                \n",
    "            return np.column_stack([proba_negative, proba_positive])\n",
    "\n",
    "class AdvancedRecommenderEnsemble:\n",
    "    \"\"\"Advanced ensemble recommender system\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_selector = None\n",
    "        \n",
    "    def create_base_models(self):\n",
    "        \"\"\"Create diverse base models\"\"\"\n",
    "        models = {}\n",
    "        \n",
    "        # 1. Logistic Regression with regularization\n",
    "        models['logistic'] = LogisticRegression(\n",
    "            random_state=self.random_state,\n",
    "            class_weight='balanced',\n",
    "            C=0.1,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "        # 2. Random Forest with tuned parameters\n",
    "        models['random_forest'] = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight='balanced_subsample',\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # 3. Gradient Boosting\n",
    "        models['gradient_boosting'] = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=8,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # 4. LightGBM if available (instead of XGBoost)\n",
    "        if LIGHTGBM_AVAILABLE:\n",
    "            models['lightgbm'] = lgb.LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=8,\n",
    "                min_child_samples=10,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                objective='binary',\n",
    "                metric='binary_logloss',\n",
    "                verbose=-1  # Suppress output\n",
    "            )\n",
    "        \n",
    "        # 5. PyTorch Neural Network if available\n",
    "        if PYTORCH_AVAILABLE:\n",
    "            models['pytorch_nn'] = PyTorchWrapper(\n",
    "                input_size=None,  # Will be set dynamically\n",
    "                hidden_sizes=[128, 64, 32],\n",
    "                learning_rate=0.001,\n",
    "                epochs=30,\n",
    "                batch_size=512\n",
    "            )\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val, feature_names):\n",
    "        \"\"\"Fit ensemble with feature selection and validation\"\"\"\n",
    "        \n",
    "        print(\"üîß Training advanced ensemble...\")\n",
    "        \n",
    "        # Feature selection\n",
    "        self.feature_selector = SelectFromModel(\n",
    "            RandomForestClassifier(n_estimators=50, random_state=self.random_state),\n",
    "            max_features=min(MAX_FEATURES_TO_SELECT, X_train.shape[1])\n",
    "        )\n",
    "        \n",
    "        X_train_selected = self.feature_selector.fit_transform(X_train, y_train)\n",
    "        X_val_selected = self.feature_selector.transform(X_val)\n",
    "        \n",
    "        selected_features = feature_names[self.feature_selector.get_support()]\n",
    "        print(f\"  ‚úì Selected {len(selected_features)} most important features\")\n",
    "        \n",
    "        # Train base models\n",
    "        self.models = self.create_base_models()\n",
    "        validation_scores = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"  üèãÔ∏è Training {name}...\")\n",
    "            \n",
    "            # Special handling for PyTorch model\n",
    "            if name == 'pytorch_nn' and PYTORCH_AVAILABLE:\n",
    "                model.input_size = X_train_selected.shape[1]\n",
    "                model.fit(X_train_selected, y_train)\n",
    "            else:\n",
    "                model.fit(X_train_selected, y_train)\n",
    "            \n",
    "            # Validate\n",
    "            val_pred = model.predict_proba(X_val_selected)[:, 1]\n",
    "            val_auc = roc_auc_score(y_val, val_pred)\n",
    "            validation_scores[name] = val_auc\n",
    "            \n",
    "            print(f\"    Validation AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Calculate ensemble weights based on validation performance\n",
    "        total_score = sum(validation_scores.values())\n",
    "        self.weights = {name: score/total_score for name, score in validation_scores.items()}\n",
    "        \n",
    "        print(f\"\\n  üìä Ensemble weights:\")\n",
    "        for name, weight in self.weights.items():\n",
    "            print(f\"    {name}: {weight:.3f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Ensemble prediction with weighted voting\"\"\"\n",
    "        X_selected = self.feature_selector.transform(X)\n",
    "        \n",
    "        predictions = np.zeros((X.shape[0], 2))\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            model_pred = model.predict_proba(X_selected)\n",
    "            predictions += self.weights[name] * model_pred\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Binary predictions\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] > 0.5).astype(int)\n",
    "\n",
    "# Train the advanced ensemble\n",
    "ensemble_model = AdvancedRecommenderEnsemble(random_state=RANDOM_STATE)\n",
    "ensemble_model.fit(X_train_scaled, y_train, X_val_scaled, y_val, X.columns)\n",
    "\n",
    "print(\"‚úÖ Advanced ensemble model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46556395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà ADVANCED EVALUATION FRAMEWORK\n",
      "==================================================\n",
      "üéØ EVALUATION RESULTS\n",
      "==============================\n",
      "\n",
      "üìä Classification Metrics:\n",
      "  AUC_ROC: 0.7220\n",
      "  AUC_PR: 0.4328\n",
      "  PRECISION: 0.4757\n",
      "  RECALL: 0.3171\n",
      "  F1: 0.3805\n",
      "\n",
      "üéØ Recommendation Metrics:\n",
      "\n",
      "  üìã Top-3 Recommendations:\n",
      "    Precision@3: 0.3961\n",
      "    Recall@3: 0.8832\n",
      "    NDCG@3: 0.7616\n",
      "\n",
      "  üìã Top-5 Recommendations:\n",
      "    Precision@5: 0.3074\n",
      "    Recall@5: 0.9730\n",
      "    NDCG@5: 0.6756\n",
      "\n",
      "  üìã Top-10 Recommendations:\n",
      "    Precision@10: 0.0000\n",
      "    Recall@10: 0.0000\n",
      "    NDCG@10: 0.0000\n",
      "\n",
      "üéØ BUSINESS TARGET ASSESSMENT:\n",
      "  Precision@3: 39.6% (Target: 60.0%) ‚ùå\n",
      "  Recall@10: 0.0% (Target: 80.0%) ‚ùå\n",
      "  NDCG@5: 67.6% (Target: 70.0%) ‚ùå\n",
      "\n",
      "üìà Overall Performance: 0/3 targets met\n",
      "‚ö†Ô∏è  Model needs further improvement before deployment\n",
      "üéØ EVALUATION RESULTS\n",
      "==============================\n",
      "\n",
      "üìä Classification Metrics:\n",
      "  AUC_ROC: 0.7220\n",
      "  AUC_PR: 0.4328\n",
      "  PRECISION: 0.4757\n",
      "  RECALL: 0.3171\n",
      "  F1: 0.3805\n",
      "\n",
      "üéØ Recommendation Metrics:\n",
      "\n",
      "  üìã Top-3 Recommendations:\n",
      "    Precision@3: 0.3961\n",
      "    Recall@3: 0.8832\n",
      "    NDCG@3: 0.7616\n",
      "\n",
      "  üìã Top-5 Recommendations:\n",
      "    Precision@5: 0.3074\n",
      "    Recall@5: 0.9730\n",
      "    NDCG@5: 0.6756\n",
      "\n",
      "  üìã Top-10 Recommendations:\n",
      "    Precision@10: 0.0000\n",
      "    Recall@10: 0.0000\n",
      "    NDCG@10: 0.0000\n",
      "\n",
      "üéØ BUSINESS TARGET ASSESSMENT:\n",
      "  Precision@3: 39.6% (Target: 60.0%) ‚ùå\n",
      "  Recall@10: 0.0% (Target: 80.0%) ‚ùå\n",
      "  NDCG@5: 67.6% (Target: 70.0%) ‚ùå\n",
      "\n",
      "üìà Overall Performance: 0/3 targets met\n",
      "‚ö†Ô∏è  Model needs further improvement before deployment\n"
     ]
    }
   ],
   "source": [
    "# Advanced Evaluation Framework\n",
    "print(\"üìà ADVANCED EVALUATION FRAMEWORK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def evaluate_recommendation_system(model, X_test, y_test, test_data, top_k=[3, 5, 10]):\n",
    "    \"\"\"Comprehensive evaluation for recommendation system\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Basic classification metrics\n",
    "    results['classification'] = {\n",
    "        'auc_roc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'auc_pr': average_precision_score(y_test, y_pred_proba),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Recommendation-specific metrics\n",
    "    results['recommendation'] = {}\n",
    "    \n",
    "    # Create user-item matrix for ranking evaluation\n",
    "    test_df = test_data.copy()\n",
    "    test_df['prediction_score'] = y_pred_proba\n",
    "    \n",
    "    users = test_df['user_id'].unique()\n",
    "    \n",
    "    precision_at_k = {}\n",
    "    recall_at_k = {}\n",
    "    ndcg_at_k = {}\n",
    "    \n",
    "    for k in top_k:\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        ndcgs = []\n",
    "        \n",
    "        for user in users:\n",
    "            user_data = test_df[test_df['user_id'] == user].copy()\n",
    "            \n",
    "            if len(user_data) < k:\n",
    "                continue\n",
    "                \n",
    "            # Sort by prediction score (descending)\n",
    "            user_data = user_data.sort_values('prediction_score', ascending=False)\n",
    "            \n",
    "            # Top-k recommendations\n",
    "            top_k_items = user_data.head(k)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            relevant_items = user_data[user_data['adopted'] == 1]\n",
    "            \n",
    "            if len(relevant_items) > 0:\n",
    "                # Precision@k\n",
    "                precision_k = len(top_k_items[top_k_items['adopted'] == 1]) / k\n",
    "                precisions.append(precision_k)\n",
    "                \n",
    "                # Recall@k\n",
    "                recall_k = len(top_k_items[top_k_items['adopted'] == 1]) / len(relevant_items)\n",
    "                recalls.append(recall_k)\n",
    "                \n",
    "                # NDCG@k\n",
    "                y_true = top_k_items['adopted'].values.reshape(1, -1)\n",
    "                y_score = top_k_items['prediction_score'].values.reshape(1, -1)\n",
    "                \n",
    "                if len(np.unique(y_true)) > 1:  # Need both 0s and 1s for NDCG\n",
    "                    ndcg_k = ndcg_score(y_true, y_score, k=k)\n",
    "                    ndcgs.append(ndcg_k)\n",
    "        \n",
    "        precision_at_k[k] = np.mean(precisions) if precisions else 0\n",
    "        recall_at_k[k] = np.mean(recalls) if recalls else 0\n",
    "        ndcg_at_k[k] = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    results['recommendation'] = {\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'recall_at_k': recall_at_k,\n",
    "        'ndcg_at_k': ndcg_at_k\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare test data for evaluation\n",
    "test_data = modeling_data[test_mask].copy()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_recommendation_system(\n",
    "    ensemble_model, X_test_scaled, y_test, test_data\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"üéØ EVALUATION RESULTS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"\\nüìä Classification Metrics:\")\n",
    "for metric, value in evaluation_results['classification'].items():\n",
    "    print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ Recommendation Metrics:\")\n",
    "for k in [3, 5, 10]:\n",
    "    print(f\"\\n  üìã Top-{k} Recommendations:\")\n",
    "    print(f\"    Precision@{k}: {evaluation_results['recommendation']['precision_at_k'][k]:.4f}\")\n",
    "    print(f\"    Recall@{k}: {evaluation_results['recommendation']['recall_at_k'][k]:.4f}\")\n",
    "    print(f\"    NDCG@{k}: {evaluation_results['recommendation']['ndcg_at_k'][k]:.4f}\")\n",
    "\n",
    "# Check if we meet business targets\n",
    "print(\"\\nüéØ BUSINESS TARGET ASSESSMENT:\")\n",
    "precision_3 = evaluation_results['recommendation']['precision_at_k'][3]\n",
    "recall_10 = evaluation_results['recommendation']['recall_at_k'][10]\n",
    "ndcg_5 = evaluation_results['recommendation']['ndcg_at_k'][5]\n",
    "\n",
    "print(f\"  Precision@3: {precision_3:.1%} (Target: {TARGET_PRECISION_AT_3:.1%}) {'‚úÖ' if precision_3 >= TARGET_PRECISION_AT_3 else '‚ùå'}\")\n",
    "print(f\"  Recall@10: {recall_10:.1%} (Target: {TARGET_RECALL_AT_10:.1%}) {'‚úÖ' if recall_10 >= TARGET_RECALL_AT_10 else '‚ùå'}\")\n",
    "print(f\"  NDCG@5: {ndcg_5:.1%} (Target: {TARGET_NDCG_AT_5:.1%}) {'‚úÖ' if ndcg_5 >= TARGET_NDCG_AT_5 else '‚ùå'}\")\n",
    "\n",
    "targets_met = sum([\n",
    "    precision_3 >= TARGET_PRECISION_AT_3,\n",
    "    recall_10 >= TARGET_RECALL_AT_10,\n",
    "    ndcg_5 >= TARGET_NDCG_AT_5\n",
    "])\n",
    "\n",
    "print(f\"\\nüìà Overall Performance: {targets_met}/3 targets met\")\n",
    "\n",
    "if targets_met >= 2:\n",
    "    print(\"üéâ Model performance is ACCEPTABLE for production deployment!\")\n",
    "    SHAP_READY = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Model needs further improvement before deployment\")\n",
    "    SHAP_READY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c4ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ GENERATING TOP-3 PRODUCT RECOMMENDATIONS\n",
      "==================================================\n",
      "Generating recommendations for 17,795 users...\n",
      "\n",
      "üìã SAMPLE RECOMMENDATIONS\n",
      "==============================\n",
      "\n",
      "üë§ User 55cb2fca-2a06-4c20-b996-9d9dac80f871:\n",
      "  1. Product_3f19d078-4239-49a9-a77b-c389f421670e (Mortgage)\n",
      "     Score: 0.438 | ‚ùå Not adopted\n",
      "  2. Product_69702bb0-34ca-4850-8d38-3d3c261d063e (SavingsAccount)\n",
      "     Score: 0.437 | ‚úÖ ADOPTED\n",
      "  3. Product_ce07d04c-b6e7-449d-a1ab-f13267ac7cf1 (FXTransfer)\n",
      "     Score: 0.437 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User aec0d92b-484d-4f8e-8fbb-1c856995452d:\n",
      "  1. Product_5c28ab1e-bb10-4348-b3ab-c8a4dfa4f0e2 (FXTransfer)\n",
      "     Score: 0.445 | ‚ùå Not adopted\n",
      "  2. Product_199c7c90-4c12-4f8e-9a1e-86b14c055774 (Insurance)\n",
      "     Score: 0.152 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User 053eb314-4483-45e3-bdc2-4982cac49fda:\n",
      "  1. Product_5c28ab1e-bb10-4348-b3ab-c8a4dfa4f0e2 (FXTransfer)\n",
      "     Score: 0.195 | ‚ùå Not adopted\n",
      "  2. Product_69702bb0-34ca-4850-8d38-3d3c261d063e (SavingsAccount)\n",
      "     Score: 0.186 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User fcc0bc5e-7ebe-4246-bed8-281dee7ae828:\n",
      "  1. Product_b8fb9bd0-c89f-4963-8abc-0c38d8b31e57 (FXTransfer)\n",
      "     Score: 0.518 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User d825888f-5e1f-4866-af06-c37da5e9aef0:\n",
      "  1. Product_f6af27e7-2dff-428f-8819-161da45655e8 (SavingsAccount)\n",
      "     Score: 0.433 | ‚úÖ ADOPTED\n",
      "  2. Product_e0d289ba-6f94-47f6-a2b2-ecdaa0de0535 (Overdraft)\n",
      "     Score: 0.428 | ‚ùå Not adopted\n",
      "\n",
      "üìä RECOMMENDATION ACCURACY\n",
      "==============================\n",
      "Overall recommendation accuracy: 20.5%\n",
      "Total recommendations generated: 244\n",
      "Successful recommendations: 50\n",
      "\n",
      "üìã SAMPLE RECOMMENDATIONS\n",
      "==============================\n",
      "\n",
      "üë§ User 55cb2fca-2a06-4c20-b996-9d9dac80f871:\n",
      "  1. Product_3f19d078-4239-49a9-a77b-c389f421670e (Mortgage)\n",
      "     Score: 0.438 | ‚ùå Not adopted\n",
      "  2. Product_69702bb0-34ca-4850-8d38-3d3c261d063e (SavingsAccount)\n",
      "     Score: 0.437 | ‚úÖ ADOPTED\n",
      "  3. Product_ce07d04c-b6e7-449d-a1ab-f13267ac7cf1 (FXTransfer)\n",
      "     Score: 0.437 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User aec0d92b-484d-4f8e-8fbb-1c856995452d:\n",
      "  1. Product_5c28ab1e-bb10-4348-b3ab-c8a4dfa4f0e2 (FXTransfer)\n",
      "     Score: 0.445 | ‚ùå Not adopted\n",
      "  2. Product_199c7c90-4c12-4f8e-9a1e-86b14c055774 (Insurance)\n",
      "     Score: 0.152 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User 053eb314-4483-45e3-bdc2-4982cac49fda:\n",
      "  1. Product_5c28ab1e-bb10-4348-b3ab-c8a4dfa4f0e2 (FXTransfer)\n",
      "     Score: 0.195 | ‚ùå Not adopted\n",
      "  2. Product_69702bb0-34ca-4850-8d38-3d3c261d063e (SavingsAccount)\n",
      "     Score: 0.186 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User fcc0bc5e-7ebe-4246-bed8-281dee7ae828:\n",
      "  1. Product_b8fb9bd0-c89f-4963-8abc-0c38d8b31e57 (FXTransfer)\n",
      "     Score: 0.518 | ‚ùå Not adopted\n",
      "\n",
      "üë§ User d825888f-5e1f-4866-af06-c37da5e9aef0:\n",
      "  1. Product_f6af27e7-2dff-428f-8819-161da45655e8 (SavingsAccount)\n",
      "     Score: 0.433 | ‚úÖ ADOPTED\n",
      "  2. Product_e0d289ba-6f94-47f6-a2b2-ecdaa0de0535 (Overdraft)\n",
      "     Score: 0.428 | ‚ùå Not adopted\n",
      "\n",
      "üìä RECOMMENDATION ACCURACY\n",
      "==============================\n",
      "Overall recommendation accuracy: 20.5%\n",
      "Total recommendations generated: 244\n",
      "Successful recommendations: 50\n"
     ]
    }
   ],
   "source": [
    "# Generate Top-3 Product Recommendations\n",
    "print(\"üéØ GENERATING TOP-3 PRODUCT RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def generate_user_recommendations(model, user_data, customers_df, products_df, top_k=3):\n",
    "    \"\"\"Generate top-k product recommendations for each user\"\"\"\n",
    "    \n",
    "    recommendations = {}\n",
    "    users = user_data['user_id'].unique()\n",
    "    \n",
    "    print(f\"Generating recommendations for {len(users):,} users...\")\n",
    "    \n",
    "    for user in users[:100]:  # Limit to first 100 users for demo\n",
    "        user_interactions = user_data[user_data['user_id'] == user].copy()\n",
    "        \n",
    "        if len(user_interactions) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get prediction scores\n",
    "        user_features = user_interactions[feature_cols]\n",
    "        user_scaled = scaler.transform(user_features)\n",
    "        scores = model.predict_proba(user_scaled)[:, 1]\n",
    "        \n",
    "        # Add scores to dataframe\n",
    "        user_interactions['recommendation_score'] = scores\n",
    "        \n",
    "        # Sort by score and get top-k\n",
    "        top_recommendations = user_interactions.sort_values(\n",
    "            'recommendation_score', ascending=False\n",
    "        ).head(top_k)\n",
    "        \n",
    "        # Format recommendations\n",
    "        user_recs = []\n",
    "        for _, row in top_recommendations.iterrows():\n",
    "            product_info = products_df[products_df['product_id'] == row['product_id']].iloc[0]\n",
    "            \n",
    "            rec = {\n",
    "                'product_id': row['product_id'],\n",
    "                'product_name': product_info.get('product_name', f\"Product_{row['product_id']}\"),\n",
    "                'product_category': product_info.get('category', 'Unknown'),\n",
    "                'recommendation_score': row['recommendation_score'],\n",
    "                'predicted_adoption_probability': row['recommendation_score'],\n",
    "                'actual_adopted': row['adopted']\n",
    "            }\n",
    "            user_recs.append(rec)\n",
    "        \n",
    "        recommendations[user] = user_recs\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "user_recommendations = generate_user_recommendations(\n",
    "    ensemble_model, test_data, customers, products, top_k=3\n",
    ")\n",
    "\n",
    "# Display sample recommendations\n",
    "print(\"\\nüìã SAMPLE RECOMMENDATIONS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "sample_users = list(user_recommendations.keys())[:5]\n",
    "for user in sample_users:\n",
    "    print(f\"\\nüë§ User {user}:\")\n",
    "    for i, rec in enumerate(user_recommendations[user], 1):\n",
    "        status = \"‚úÖ ADOPTED\" if rec['actual_adopted'] else \"‚ùå Not adopted\"\n",
    "        print(f\"  {i}. {rec['product_name']} ({rec['product_category']})\")\n",
    "        print(f\"     Score: {rec['recommendation_score']:.3f} | {status}\")\n",
    "\n",
    "# Calculate recommendation accuracy\n",
    "print(f\"\\nüìä RECOMMENDATION ACCURACY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "total_recs = 0\n",
    "correct_recs = 0\n",
    "\n",
    "for user, recs in user_recommendations.items():\n",
    "    for rec in recs:\n",
    "        total_recs += 1\n",
    "        if rec['actual_adopted']:\n",
    "            correct_recs += 1\n",
    "\n",
    "recommendation_accuracy = correct_recs / total_recs if total_recs > 0 else 0\n",
    "print(f\"Overall recommendation accuracy: {recommendation_accuracy:.1%}\")\n",
    "print(f\"Total recommendations generated: {total_recs:,}\")\n",
    "print(f\"Successful recommendations: {correct_recs:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9914a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVING ADVANCED MODEL AND RESULTS\n",
      "==================================================\n",
      "‚úÖ Ensemble model saved to: model/advanced_ensemble_recommender.joblib\n",
      "‚úÖ Feature scaler saved to: model/advanced_feature_scaler.joblib\n",
      "‚úÖ Evaluation results saved to: model/advanced_evaluation_results.json\n",
      "‚úÖ Advanced model metadata saved to: model/advanced_model_metadata.json\n",
      "\n",
      "üéâ ADVANCED MODELING COMPLETED!\n",
      "üìä Final Performance Summary:\n",
      "  - Classification AUC: 0.722\n",
      "  - Precision@3: 39.6%\n",
      "  - Recall@10: 0.0%\n",
      "  - NDCG@5: 67.6%\n",
      "  - Business targets met: 0/3\n",
      "  - Dataset size: 200,000 samples\n",
      "  - Features used: 118 total, 28 selected\n",
      "\n",
      "üí° RECOMMENDATIONS FOR IMPROVEMENT:\n",
      "  1. Increase dataset size (currently using 200k sample)\n",
      "  2. Add more feature engineering (interaction terms)\n",
      "  3. Hyperparameter tuning for ensemble models\n",
      "  4. Try different sampling strategies for imbalanced data\n",
      "  5. Consider matrix factorization techniques\n",
      "  6. Use the full dataset instead of sample\n",
      "‚úÖ Ensemble model saved to: model/advanced_ensemble_recommender.joblib\n",
      "‚úÖ Feature scaler saved to: model/advanced_feature_scaler.joblib\n",
      "‚úÖ Evaluation results saved to: model/advanced_evaluation_results.json\n",
      "‚úÖ Advanced model metadata saved to: model/advanced_model_metadata.json\n",
      "\n",
      "üéâ ADVANCED MODELING COMPLETED!\n",
      "üìä Final Performance Summary:\n",
      "  - Classification AUC: 0.722\n",
      "  - Precision@3: 39.6%\n",
      "  - Recall@10: 0.0%\n",
      "  - NDCG@5: 67.6%\n",
      "  - Business targets met: 0/3\n",
      "  - Dataset size: 200,000 samples\n",
      "  - Features used: 118 total, 28 selected\n",
      "\n",
      "üí° RECOMMENDATIONS FOR IMPROVEMENT:\n",
      "  1. Increase dataset size (currently using 200k sample)\n",
      "  2. Add more feature engineering (interaction terms)\n",
      "  3. Hyperparameter tuning for ensemble models\n",
      "  4. Try different sampling strategies for imbalanced data\n",
      "  5. Consider matrix factorization techniques\n",
      "  6. Use the full dataset instead of sample\n"
     ]
    }
   ],
   "source": [
    "# Save Advanced Model and Results (Simplified)\n",
    "print(\"üíæ SAVING ADVANCED MODEL AND RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save the ensemble model\n",
    "model_save_path = 'model/advanced_ensemble_recommender.joblib'\n",
    "joblib.dump(ensemble_model, model_save_path)\n",
    "print(f\"‚úÖ Ensemble model saved to: {model_save_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_save_path = 'model/advanced_feature_scaler.joblib'\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "print(f\"‚úÖ Feature scaler saved to: {scaler_save_path}\")\n",
    "\n",
    "# Save evaluation results (simplified)\n",
    "results_save_path = 'model/advanced_evaluation_results.json'\n",
    "simple_results = {\n",
    "    'auc_roc': float(evaluation_results['classification']['auc_roc']),\n",
    "    'precision_at_3': float(evaluation_results['recommendation']['precision_at_k'][3]),\n",
    "    'recall_at_10': float(evaluation_results['recommendation']['recall_at_k'][10]),\n",
    "    'ndcg_at_5': float(evaluation_results['recommendation']['ndcg_at_k'][5]),\n",
    "    'targets_met': int(targets_met)\n",
    "}\n",
    "with open(results_save_path, 'w') as f:\n",
    "    json.dump(simple_results, f, indent=2)\n",
    "print(f\"‚úÖ Evaluation results saved to: {results_save_path}\")\n",
    "\n",
    "# Save simple metadata\n",
    "metadata_save_path = 'model/advanced_model_metadata.json'\n",
    "advanced_metadata = {\n",
    "    'model_type': 'Advanced Ensemble Recommender',\n",
    "    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'models_used': list(ensemble_model.models.keys()),\n",
    "    'features_selected': int(ensemble_model.feature_selector.get_support().sum()),\n",
    "    'total_features': int(len(feature_cols)),\n",
    "    'auc_roc': float(evaluation_results['classification']['auc_roc']),\n",
    "    'precision_at_3': float(evaluation_results['recommendation']['precision_at_k'][3]),\n",
    "    'targets_met': int(targets_met),\n",
    "    'dataset_size': int(modeling_data.shape[0]),\n",
    "    'adoption_rate': float(modeling_data['adopted'].mean())\n",
    "}\n",
    "\n",
    "with open(metadata_save_path, 'w') as f:\n",
    "    json.dump(advanced_metadata, f, indent=2)\n",
    "print(f\"‚úÖ Advanced model metadata saved to: {metadata_save_path}\")\n",
    "\n",
    "print(f\"\\nüéâ ADVANCED MODELING COMPLETED!\")\n",
    "print(f\"üìä Final Performance Summary:\")\n",
    "print(f\"  - Classification AUC: {evaluation_results['classification']['auc_roc']:.3f}\")\n",
    "print(f\"  - Precision@3: {evaluation_results['recommendation']['precision_at_k'][3]:.1%}\")\n",
    "print(f\"  - Recall@10: {evaluation_results['recommendation']['recall_at_k'][10]:.1%}\")\n",
    "print(f\"  - NDCG@5: {evaluation_results['recommendation']['ndcg_at_k'][5]:.1%}\")\n",
    "print(f\"  - Business targets met: {targets_met}/3\")\n",
    "print(f\"  - Dataset size: {modeling_data.shape[0]:,} samples\")\n",
    "print(f\"  - Features used: {len(feature_cols)} total, {ensemble_model.feature_selector.get_support().sum()} selected\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
    "print(f\"  1. Increase dataset size (currently using 200k sample)\")\n",
    "print(f\"  2. Add more feature engineering (interaction terms)\")\n",
    "print(f\"  3. Hyperparameter tuning for ensemble models\")\n",
    "print(f\"  4. Try different sampling strategies for imbalanced data\")\n",
    "print(f\"  5. Consider matrix factorization techniques\")\n",
    "print(f\"  6. Use the full dataset instead of sample\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
