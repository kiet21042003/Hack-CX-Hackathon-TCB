{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466cfca3",
   "metadata": {},
   "source": [
    "# II. Baseline Modeling: Customer Product Adoption Prediction\n",
    "\n",
    "## Objectives\n",
    "- Establish baseline model performance benchmarks\n",
    "- Compare multiple algorithms for product adoption prediction\n",
    "- Implement proper evaluation framework with business metrics\n",
    "- Create model interpretation and feature importance analysis\n",
    "- Prepare foundation for advanced modeling techniques\n",
    "\n",
    "## Modeling Strategy\n",
    "We'll implement several baseline models:\n",
    "1. **Logistic Regression** - Linear baseline with interpretability\n",
    "2. **Random Forest** - Ensemble method for feature importance\n",
    "3. **XGBoost** - Gradient boosting for performance\n",
    "4. **Neural Network** - Deep learning baseline\n",
    "5. **Naive Bayes** - Probabilistic baseline\n",
    "\n",
    "## Evaluation Framework\n",
    "- **Primary Metrics**: Precision@K, Recall@K, F1-Score\n",
    "- **Business Metrics**: Conversion Lift, Revenue Impact\n",
    "- **Statistical Tests**: McNemar's test for model comparison\n",
    "- **Cross-Validation**: Time-based splits to prevent leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449a8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è XGBoost not available. Install with: pip install xgboost\n",
      "üöÄ Baseline Modeling Environment Initialized\n",
      "XGBoost Available: False\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, roc_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Advanced ML Libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"üöÄ Baseline Modeling Environment Initialized\")\n",
    "print(f\"XGBoost Available: {XGBOOST_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2da29",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dabe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LOADING PROCESSED DATASETS\n",
      "==================================================\n",
      "‚úì Loaded processed adoption logs: (949650, 12)\n",
      "‚úì Loaded processed products: (1000, 70)\n",
      "‚ö†Ô∏è Selected features file not found. Will use all features.\n",
      "\n",
      "Preprocessed data available: True\n",
      "Feature selection available: False\n",
      "‚úì Loaded processed adoption logs: (949650, 12)\n",
      "‚úì Loaded processed products: (1000, 70)\n",
      "‚ö†Ô∏è Selected features file not found. Will use all features.\n",
      "\n",
      "Preprocessed data available: True\n",
      "Feature selection available: False\n"
     ]
    }
   ],
   "source": [
    "# Load processed datasets from EDA\n",
    "print(\"üìä LOADING PROCESSED DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Try to load processed datasets first\n",
    "    adoption_data = pd.read_csv('data/processed_adoption_logs.csv')\n",
    "    print(f\"‚úì Loaded processed adoption logs: {adoption_data.shape}\")\n",
    "    \n",
    "    products_data = pd.read_csv('data/processed_products.csv')\n",
    "    print(f\"‚úì Loaded processed products: {products_data.shape}\")\n",
    "    \n",
    "    PROCESSED_DATA_AVAILABLE = True\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Processed datasets not found. Loading raw data...\")\n",
    "    PROCESSED_DATA_AVAILABLE = False\n",
    "    \n",
    "    # Load raw datasets\n",
    "    try:\n",
    "        adoption_data = pd.read_csv('data/data_adoption_logs.csv')\n",
    "        products_data = pd.read_csv('data/data_products.csv')\n",
    "        customers_data = pd.read_csv('data/data_customers.csv')\n",
    "        \n",
    "        print(f\"‚úì Loaded raw adoption logs: {adoption_data.shape}\")\n",
    "        print(f\"‚úì Loaded raw products: {products_data.shape}\")\n",
    "        print(f\"‚úì Loaded raw customers: {customers_data.shape}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Error loading datasets: {e}\")\n",
    "        print(\"Please ensure data files are available in the data/ directory\")\n",
    "\n",
    "# Load feature selection if available\n",
    "try:\n",
    "    with open('selected_features.txt', 'r') as f:\n",
    "        selected_features = [line.strip() for line in f.readlines()]\n",
    "    print(f\"‚úì Loaded {len(selected_features)} selected features\")\n",
    "    FEATURE_SELECTION_AVAILABLE = True\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Selected features file not found. Will use all features.\")\n",
    "    FEATURE_SELECTION_AVAILABLE = False\n",
    "    selected_features = []\n",
    "\n",
    "print(f\"\\nPreprocessed data available: {PROCESSED_DATA_AVAILABLE}\")\n",
    "print(f\"Feature selection available: {FEATURE_SELECTION_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6a09fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß PREPARING MODELING DATASET\n",
      "========================================\n",
      "Using processed adoption logs as base dataset\n",
      "\n",
      "ModelingDataset prepared:\n",
      "  Features: 11\n",
      "  Samples: 949,650\n",
      "  Target distribution: {0: 711603, 1: 238047}\n",
      "  Positive rate: 0.2507\n"
     ]
    }
   ],
   "source": [
    "# Prepare modeling dataset\n",
    "print(\"\\nüîß PREPARING MODELING DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if PROCESSED_DATA_AVAILABLE:\n",
    "    # Use processed data\n",
    "    modeling_data = adoption_data.copy()\n",
    "    print(\"Using processed adoption logs as base dataset\")\n",
    "    \n",
    "else:\n",
    "    # Quick preprocessing for raw data\n",
    "    print(\"Applying basic preprocessing to raw data...\")\n",
    "    \n",
    "    modeling_data = adoption_data.copy()\n",
    "    \n",
    "    # Basic encoding for categorical columns\n",
    "    categorical_cols = modeling_data.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col not in ['user_id', 'product_id']]\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if modeling_data[col].nunique() <= 10:\n",
    "            # One-hot encode low cardinality\n",
    "            dummies = pd.get_dummies(modeling_data[col], prefix=col, drop_first=True)\n",
    "            modeling_data = pd.concat([modeling_data.drop(columns=[col]), dummies], axis=1)\n",
    "        else:\n",
    "            # Drop high cardinality categorical\n",
    "            modeling_data = modeling_data.drop(columns=[col])\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_cols = modeling_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        if modeling_data[col].isnull().sum() > 0:\n",
    "            modeling_data[col].fillna(modeling_data[col].median(), inplace=True)\n",
    "\n",
    "# Prepare features and target\n",
    "if 'adopted' in modeling_data.columns:\n",
    "    target_col = 'adopted'\n",
    "    \n",
    "    # Remove ID columns from features\n",
    "    id_cols = ['user_id', 'product_id']\n",
    "    feature_cols = [col for col in modeling_data.columns if col not in [target_col] + id_cols]\n",
    "    \n",
    "    # Apply feature selection if available\n",
    "    if FEATURE_SELECTION_AVAILABLE and selected_features:\n",
    "        available_selected = [f for f in selected_features if f in feature_cols]\n",
    "        if available_selected:\n",
    "            feature_cols = available_selected\n",
    "            print(f\"Applied feature selection: {len(feature_cols)} features\")\n",
    "    \n",
    "    X = modeling_data[feature_cols]\n",
    "    y = modeling_data[target_col]\n",
    "    \n",
    "    # Convert boolean target to integer\n",
    "    if y.dtype == 'bool':\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    print(f\"\\nModelingDataset prepared:\")\n",
    "    print(f\"  Features: {X.shape[1]}\")\n",
    "    print(f\"  Samples: {X.shape[0]:,}\")\n",
    "    print(f\"  Target distribution: {y.value_counts().to_dict()}\")\n",
    "    print(f\"  Positive rate: {y.mean():.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Target variable 'adopted' not found in dataset\")\n",
    "    X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2f9b3",
   "metadata": {},
   "source": [
    "## 2. Data Splitting and Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2c0c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÄ IMPLEMENTING VALIDATION STRATEGY\n",
      "========================================\n",
      "Using tenure_days for time-based validation split\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 58.0 MiB for an array with shape (10, 759720) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m train_data = modeling_data_sorted.iloc[:split_idx]\n\u001b[32m     20\u001b[39m test_data = modeling_data_sorted.iloc[split_idx:]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m X_train = \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m y_train = train_data[target_col].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     24\u001b[39m X_test = test_data[feature_cols]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4117\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   4115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._slice(indexer, axis=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4117\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4120\u001b[39m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[32m   4121\u001b[39m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[32m   4122\u001b[39m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[32m   4123\u001b[39m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[32m   4124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n\u001b[32m   4125\u001b[39m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4153\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4142\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4145\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4146\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4151\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4153\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4154\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4133\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4128\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4129\u001b[39m     indices = np.arange(\n\u001b[32m   4130\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4131\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4133\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4135\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4139\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4140\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    891\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m    688\u001b[39m         blk.take_nd(\n\u001b[32m    689\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    841\u001b[39m                     blocks.append(nb)\n\u001b[32m    842\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m                 nb = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m                 blocks.append(nb)\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1304\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1312\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1314\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1315\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    155\u001b[39m     out = np.empty(out_shape, dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m    162\u001b[39m func(arr, indexer, out, fill_value)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 58.0 MiB for an array with shape (10, 759720) and data type float64"
     ]
    }
   ],
   "source": [
    "# Implement time-based data splitting for realistic evaluation\n",
    "print(\"üîÄ IMPLEMENTING VALIDATION STRATEGY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    # Check for temporal columns for time-based split\n",
    "    temporal_cols = [col for col in modeling_data.columns if 'date' in col.lower() or 'days' in col.lower()]\n",
    "    \n",
    "    if 'tenure_days' in modeling_data.columns:\n",
    "        # Use tenure_days for time-based split\n",
    "        print(\"Using tenure_days for time-based validation split\")\n",
    "        \n",
    "        # Sort by tenure to simulate temporal order\n",
    "        modeling_data_sorted = modeling_data.sort_values('tenure_days')\n",
    "        \n",
    "        # Use last 20% as test set (most recent interactions)\n",
    "        split_idx = int(0.8 * len(modeling_data_sorted))\n",
    "        \n",
    "        train_data = modeling_data_sorted.iloc[:split_idx]\n",
    "        test_data = modeling_data_sorted.iloc[split_idx:]\n",
    "        \n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data[target_col].astype(int)\n",
    "        X_test = test_data[feature_cols]\n",
    "        y_test = test_data[target_col].astype(int)\n",
    "        \n",
    "        print(f\"Time-based split applied:\")\n",
    "        print(f\"  Train set: {X_train.shape[0]:,} samples\")\n",
    "        print(f\"  Test set: {X_test.shape[0]:,} samples\")\n",
    "        print(f\"  Train positive rate: {y_train.mean():.4f}\")\n",
    "        print(f\"  Test positive rate: {y_test.mean():.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # Standard stratified split\n",
    "        print(\"Using stratified random split\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Stratified split applied:\")\n",
    "        print(f\"  Train set: {X_train.shape[0]:,} samples\")\n",
    "        print(f\"  Test set: {X_test.shape[0]:,} samples\")\n",
    "        print(f\"  Train positive rate: {y_train.mean():.4f}\")\n",
    "        print(f\"  Test positive rate: {y_test.mean():.4f}\")\n",
    "    \n",
    "    # Create validation set from training data\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal data splits:\")\n",
    "    print(f\"  Training: {X_train_final.shape[0]:,} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]:,} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]:,} samples\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"‚úì Feature scaling applied\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed with modeling - data preparation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e292d9",
   "metadata": {},
   "source": [
    "## 3. Baseline Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train baseline models\n",
    "print(\"ü§ñ TRAINING BASELINE MODELS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    # Initialize models\n",
    "    models = {}\n",
    "    model_results = {}\n",
    "    \n",
    "    # 1. Logistic Regression\n",
    "    print(\"\\n1Ô∏è‚É£ Training Logistic Regression...\")\n",
    "    models['logistic'] = LogisticRegression(\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000\n",
    "    )\n",
    "    models['logistic'].fit(X_train_scaled, y_train_final)\n",
    "    print(\"   ‚úì Logistic Regression trained\")\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"\\n2Ô∏è‚É£ Training Random Forest...\")\n",
    "    models['random_forest'] = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    models['random_forest'].fit(X_train_final, y_train_final)\n",
    "    print(\"   ‚úì Random Forest trained\")\n",
    "    \n",
    "    # 3. XGBoost (if available)\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        print(\"\\n3Ô∏è‚É£ Training XGBoost...\")\n",
    "        \n",
    "        # Calculate scale_pos_weight for class imbalance\n",
    "        scale_pos_weight = (y_train_final == 0).sum() / (y_train_final == 1).sum()\n",
    "        \n",
    "        models['xgboost'] = xgb.XGBClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        models['xgboost'].fit(X_train_final, y_train_final)\n",
    "        print(\"   ‚úì XGBoost trained\")\n",
    "    \n",
    "    # 4. Naive Bayes\n",
    "    print(\"\\n4Ô∏è‚É£ Training Naive Bayes...\")\n",
    "    models['naive_bayes'] = GaussianNB()\n",
    "    models['naive_bayes'].fit(X_train_scaled, y_train_final)\n",
    "    print(\"   ‚úì Naive Bayes trained\")\n",
    "    \n",
    "    # 5. Neural Network\n",
    "    print(\"\\n5Ô∏è‚É£ Training Neural Network...\")\n",
    "    models['neural_network'] = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    models['neural_network'].fit(X_train_scaled, y_train_final)\n",
    "    print(\"   ‚úì Neural Network trained\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Trained {len(models)} baseline models\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot train models - data not available\")\n",
    "    models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf954856",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff401ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "print(\"üìä COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if models and X is not None:\n",
    "    evaluation_results = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nüîç Evaluating {model_name.upper()}...\")\n",
    "        \n",
    "        # Use scaled features for models that need them\n",
    "        if model_name in ['logistic', 'naive_bayes', 'neural_network']:\n",
    "            val_features = X_val_scaled\n",
    "            test_features = X_test_scaled\n",
    "        else:\n",
    "            val_features = X_val\n",
    "            test_features = X_test\n",
    "        \n",
    "        # Validation predictions\n",
    "        y_val_pred = model.predict(val_features)\n",
    "        y_val_proba = model.predict_proba(val_features)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Test predictions\n",
    "        y_test_pred = model.predict(test_features)\n",
    "        y_test_proba = model.predict_proba(test_features)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Store predictions\n",
    "        predictions[model_name] = {\n",
    "            'val_pred': y_val_pred,\n",
    "            'val_proba': y_val_proba,\n",
    "            'test_pred': y_test_pred,\n",
    "            'test_proba': y_test_proba\n",
    "        }\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_metrics = {\n",
    "            'precision': precision_score(y_val, y_val_pred),\n",
    "            'recall': recall_score(y_val, y_val_pred),\n",
    "            'f1': f1_score(y_val, y_val_pred),\n",
    "            'roc_auc': roc_auc_score(y_val, y_val_proba) if y_val_proba is not None else None\n",
    "        }\n",
    "        \n",
    "        test_metrics = {\n",
    "            'precision': precision_score(y_test, y_test_pred),\n",
    "            'recall': recall_score(y_test, y_test_pred),\n",
    "            'f1': f1_score(y_test, y_test_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_test_proba) if y_test_proba is not None else None\n",
    "        }\n",
    "        \n",
    "        evaluation_results[model_name] = {\n",
    "            'validation': val_metrics,\n",
    "            'test': test_metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"   Validation - Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "        print(f\"   Test - Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model evaluation completed\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot evaluate models - models not trained\")\n",
    "    evaluation_results = {}\n",
    "    predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c61210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results comparison\n",
    "if evaluation_results:\n",
    "    print(\"\\nüìà MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_data = []\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        for split, metrics in results.items():\n",
    "            row = {'Model': model_name, 'Split': split}\n",
    "            row.update(metrics)\n",
    "            results_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Display results table\n",
    "    print(\"\\nüìä Detailed Results:\")\n",
    "    pivot_results = results_df.pivot_table(\n",
    "        index='Model', \n",
    "        columns='Split', \n",
    "        values=['precision', 'recall', 'f1', 'roc_auc'],\n",
    "        aggfunc='first'\n",
    "    ).round(4)\n",
    "    \n",
    "    display(pivot_results)\n",
    "    \n",
    "    # Identify best model\n",
    "    test_f1_scores = results_df[results_df['Split'] == 'test']['f1']\n",
    "    best_model_idx = test_f1_scores.idxmax()\n",
    "    best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "    best_f1 = results_df.loc[best_model_idx, 'f1']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model_name.upper()} (Test F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Precision Comparison', 'Recall Comparison', 'F1-Score Comparison', 'ROC-AUC Comparison')\n",
    "    )\n",
    "    \n",
    "    test_results = results_df[results_df['Split'] == 'test']\n",
    "    \n",
    "    # Precision\n",
    "    fig.add_trace(go.Bar(x=test_results['Model'], y=test_results['precision'], name='Precision'), row=1, col=1)\n",
    "    \n",
    "    # Recall\n",
    "    fig.add_trace(go.Bar(x=test_results['Model'], y=test_results['recall'], name='Recall'), row=1, col=2)\n",
    "    \n",
    "    # F1-Score\n",
    "    fig.add_trace(go.Bar(x=test_results['Model'], y=test_results['f1'], name='F1-Score'), row=2, col=1)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    roc_auc_data = test_results.dropna(subset=['roc_auc'])\n",
    "    fig.add_trace(go.Bar(x=roc_auc_data['Model'], y=roc_auc_data['roc_auc'], name='ROC-AUC'), row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Model Performance Comparison (Test Set)\", showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d99ef6",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if models and X is not None:\n",
    "    feature_importance_results = {}\n",
    "    \n",
    "    # Random Forest Feature Importance\n",
    "    if 'random_forest' in models:\n",
    "        rf_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': models['random_forest'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        feature_importance_results['random_forest'] = rf_importance\n",
    "        \n",
    "        print(\"\\nüå≤ Random Forest - Top 10 Important Features:\")\n",
    "        for i, row in rf_importance.head(10).iterrows():\n",
    "            print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # XGBoost Feature Importance\n",
    "    if 'xgboost' in models and XGBOOST_AVAILABLE:\n",
    "        xgb_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': models['xgboost'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        feature_importance_results['xgboost'] = xgb_importance\n",
    "        \n",
    "        print(\"\\n‚ö° XGBoost - Top 10 Important Features:\")\n",
    "        for i, row in xgb_importance.head(10).iterrows():\n",
    "            print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Logistic Regression Coefficients\n",
    "    if 'logistic' in models:\n",
    "        lr_coefs = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'coefficient': models['logistic'].coef_[0]\n",
    "        })\n",
    "        lr_coefs['abs_coefficient'] = lr_coefs['coefficient'].abs()\n",
    "        lr_coefs = lr_coefs.sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        feature_importance_results['logistic'] = lr_coefs\n",
    "        \n",
    "        print(\"\\nüìà Logistic Regression - Top 10 Important Features (by |coefficient|):\")\n",
    "        for i, row in lr_coefs.head(10).iterrows():\n",
    "            print(f\"   {row['feature']}: {row['coefficient']:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance (Random Forest)\n",
    "    if 'random_forest' in feature_importance_results:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_features = feature_importance_results['random_forest'].head(15)\n",
    "        sns.barplot(data=top_features, x='importance', y='feature')\n",
    "        plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Feature importance analysis completed\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot analyze feature importance - models not available\")\n",
    "    feature_importance_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11552e",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation and Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e94b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business-focused model interpretation\n",
    "print(\"üíº BUSINESS INSIGHTS AND MODEL INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if evaluation_results and feature_importance_results:\n",
    "    business_insights = []\n",
    "    \n",
    "    # Performance insights\n",
    "    print(\"\\nüìä Performance Insights:\")\n",
    "    \n",
    "    best_model_name = best_model_name if 'best_model_name' in locals() else list(models.keys())[0]\n",
    "    best_model_results = evaluation_results[best_model_name]['test']\n",
    "    \n",
    "    precision = best_model_results['precision']\n",
    "    recall = best_model_results['recall']\n",
    "    f1 = best_model_results['f1']\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Best performing model: {best_model_name}\")\n",
    "    print(f\"   ‚Ä¢ Precision: {precision:.4f} ({precision*100:.1f}% of predicted adoptions are correct)\")\n",
    "    print(f\"   ‚Ä¢ Recall: {recall:.4f} ({recall*100:.1f}% of actual adoptions are captured)\")\n",
    "    print(f\"   ‚Ä¢ F1-Score: {f1:.4f} (balanced precision-recall performance)\")\n",
    "    \n",
    "    business_insights.append(f\"Best model achieves {precision*100:.1f}% precision and {recall*100:.1f}% recall\")\n",
    "    \n",
    "    # Feature insights\n",
    "    if 'random_forest' in feature_importance_results:\n",
    "        print(\"\\nüéØ Key Predictive Features:\")\n",
    "        top_features = feature_importance_results['random_forest'].head(5)\n",
    "        \n",
    "        for i, row in top_features.iterrows():\n",
    "            feature_name = row['feature']\n",
    "            importance = row['importance']\n",
    "            print(f\"   ‚Ä¢ {feature_name}: {importance:.4f} importance\")\n",
    "            \n",
    "            # Interpret feature meaning\n",
    "            if 'monetary' in feature_name.lower():\n",
    "                interpretation = \"Customer spending behavior is a strong predictor\"\n",
    "            elif 'activity' in feature_name.lower():\n",
    "                interpretation = \"Customer engagement level drives adoption\"\n",
    "            elif 'tenure' in feature_name.lower():\n",
    "                interpretation = \"Customer relationship length affects adoption likelihood\"\n",
    "            elif 'risk' in feature_name.lower():\n",
    "                interpretation = \"Risk profile influences product adoption decisions\"\n",
    "            else:\n",
    "                interpretation = \"Significant predictor of adoption behavior\"\n",
    "            \n",
    "            print(f\"     ‚Üí {interpretation}\")\n",
    "            business_insights.append(f\"{feature_name} is a key predictor: {interpretation}\")\n",
    "    \n",
    "    # Business impact estimation\n",
    "    print(\"\\nüí∞ Estimated Business Impact:\")\n",
    "    \n",
    "    if 'y_test' in locals():\n",
    "        total_customers = len(y_test)\n",
    "        actual_adoptions = y_test.sum()\n",
    "        \n",
    "        # Calculate potential impact with best model\n",
    "        if best_model_name in predictions:\n",
    "            predicted_adoptions = predictions[best_model_name]['test_pred'].sum()\n",
    "            true_positives = ((predictions[best_model_name]['test_pred'] == 1) & (y_test == 1)).sum()\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Total customers evaluated: {total_customers:,}\")\n",
    "            print(f\"   ‚Ä¢ Actual adoptions: {actual_adoptions:,}\")\n",
    "            print(f\"   ‚Ä¢ Model predictions: {predicted_adoptions:,}\")\n",
    "            print(f\"   ‚Ä¢ Correctly identified adoptions: {true_positives:,}\")\n",
    "            \n",
    "            # Estimate targeting efficiency\n",
    "            if predicted_adoptions > 0:\n",
    "                targeting_efficiency = true_positives / predicted_adoptions\n",
    "                print(f\"   ‚Ä¢ Targeting efficiency: {targeting_efficiency:.2%}\")\n",
    "                \n",
    "                # Estimate cost savings (assuming targeting costs)\n",
    "                cost_per_target = 10  # Example cost per customer targeted\n",
    "                baseline_cost = total_customers * cost_per_target\n",
    "                optimized_cost = predicted_adoptions * cost_per_target\n",
    "                cost_savings = baseline_cost - optimized_cost\n",
    "                \n",
    "                print(f\"   ‚Ä¢ Estimated cost savings: ${cost_savings:,.0f} (vs. targeting all customers)\")\n",
    "                business_insights.append(f\"Model-based targeting could save ${cost_savings:,.0f} in marketing costs\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüéØ Strategic Recommendations:\")\n",
    "    recommendations = [\n",
    "        f\"Deploy {best_model_name} model for customer targeting\",\n",
    "        \"Focus marketing on high-importance feature segments\",\n",
    "        \"Implement A/B testing to validate model performance\",\n",
    "        \"Monitor model performance for drift over time\",\n",
    "        \"Collect additional data on top predictive features\"\n",
    "    ]\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    # Store insights\n",
    "    business_summary = {\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'best_model': best_model_name,\n",
    "        'performance_metrics': best_model_results,\n",
    "        'key_insights': business_insights,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "    \n",
    "    with open('baseline_modeling_insights.json', 'w') as f:\n",
    "        json.dump(business_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\nüìÅ Business insights saved to 'baseline_modeling_insights.json'\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot generate business insights - evaluation results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a13d46",
   "metadata": {},
   "source": [
    "## 7. Model Persistence and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and prepare for next steps\n",
    "print(\"üíæ MODEL PERSISTENCE AND DEPLOYMENT PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if models:\n",
    "    # Save best model\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    # Create model package\n",
    "    model_package = {\n",
    "        'model': best_model,\n",
    "        'scaler': scaler if best_model_name in ['logistic', 'naive_bayes', 'neural_network'] else None,\n",
    "        'feature_columns': feature_cols,\n",
    "        'model_type': best_model_name,\n",
    "        'performance_metrics': evaluation_results[best_model_name]['test'],\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save model package\n",
    "    with open(f'best_model_{best_model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(model_package, f)\n",
    "    \n",
    "    print(f\"‚úÖ Best model ({best_model_name}) saved to 'best_model_{best_model_name}.pkl'\")\n",
    "    \n",
    "    # Save all models for comparison\n",
    "    all_models_package = {\n",
    "        'models': models,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_cols,\n",
    "        'evaluation_results': evaluation_results,\n",
    "        'feature_importance': feature_importance_results,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open('all_baseline_models.pkl', 'wb') as f:\n",
    "        pickle.dump(all_models_package, f)\n",
    "    \n",
    "    print(\"‚úÖ All baseline models saved to 'all_baseline_models.pkl'\")\n",
    "    \n",
    "    # Create model summary report\n",
    "    model_summary = {\n",
    "        'baseline_modeling_complete': True,\n",
    "        'models_trained': list(models.keys()),\n",
    "        'best_model': best_model_name,\n",
    "        'best_model_metrics': evaluation_results[best_model_name]['test'],\n",
    "        'dataset_info': {\n",
    "            'total_samples': len(X),\n",
    "            'features': len(feature_cols),\n",
    "            'positive_rate': float(y.mean())\n",
    "        },\n",
    "        'next_steps': [\n",
    "            \"Implement advanced feature engineering\",\n",
    "            \"Experiment with ensemble methods\",\n",
    "            \"Optimize hyperparameters\",\n",
    "            \"Deploy model for A/B testing\",\n",
    "            \"Set up monitoring and retraining pipeline\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open('baseline_modeling_summary.json', 'w') as f:\n",
    "        json.dump(model_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"‚úÖ Modeling summary saved to 'baseline_modeling_summary.json'\")\n",
    "\n",
    "print(\"\\nüéØ NEXT STEPS FOR ADVANCED MODELING:\")\n",
    "print(\"=\" * 40)\n",
    "next_steps = [\n",
    "    \"1. Advanced Feature Engineering\",\n",
    "    \"   - Create interaction features between customer and product attributes\",\n",
    "    \"   - Implement temporal features and seasonality patterns\",\n",
    "    \"   - Develop customer lifetime value predictions\",\n",
    "    \"\",\n",
    "    \"2. Model Optimization\",\n",
    "    \"   - Hyperparameter tuning with GridSearch/RandomSearch\",\n",
    "    \"   - Ensemble methods (Voting, Stacking, Blending)\",\n",
    "    \"   - Advanced algorithms (LightGBM, CatBoost)\",\n",
    "    \"\",\n",
    "    \"3. Evaluation Enhancement\", \n",
    "    \"   - Business-specific metrics (Revenue lift, Customer acquisition cost)\",\n",
    "    \"   - Time-series validation for temporal robustness\",\n",
    "    \"   - Fairness and bias analysis\",\n",
    "    \"\",\n",
    "    \"4. Production Deployment\",\n",
    "    \"   - Model serving infrastructure\",\n",
    "    \"   - A/B testing framework\", \n",
    "    \"   - Monitoring and alerting system\",\n",
    "    \"   - Automated retraining pipeline\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print(\"\\nüöÄ Ready to proceed with advanced modeling techniques!\")\n",
    "print(\"üìä Baseline established - all models trained and evaluated\")\n",
    "print(\"‚úÖ Foundation prepared for production deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
